{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should preprocess Classics and News corpora:\n",
    "* split each one into individual sentences;\n",
    "* tokenize each one and replace named entities with special tokens;\n",
    "* calculate, how much dictionaries diverge (I really hope that they are almost the same)\n",
    "* learn joint BPEs\n",
    "* apply learnt BPEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../data/classics/*.txt >> ../data/generated/classics.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classics = open('../data/generated/classics.txt', encoding='utf-8').read().splitlines()\n",
    "news = open('../data/news.2016.ru.shuffled', encoding='utf-8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/universome/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "num_threads = 20\n",
    "nltk.download('punkt')\n",
    "        \n",
    "with mp.Pool(processes=num_threads) as pool:\n",
    "    # Unfortunately, we do not have russian punkt in nltk\n",
    "    # Let's hope that english will do the job\n",
    "    classics_sents = pool.map(sent_tokenize, classics)\n",
    "    news_sents = pool.map(sent_tokenize, news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences before: 311458 7159447\n",
      "Num sentences after : 709647 7476194\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize results are in inconvenient format. Let's fix this\n",
    "print('Num sentences before:', len(classics), len(news))\n",
    "classics = [s for l in classics_sents for s in (l if type(l) is list else [l])]\n",
    "news = [s for l in news_sents for s in (l if type(l) is list else [l])]\n",
    "print('Num sentences after :', len(classics), len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/generated/classics.split', 'w', encoding='utf-8') as out_f:\n",
    "    for line in classics:\n",
    "        out_f.write(line + '\\n')\n",
    "        \n",
    "with open('../data/generated/news.ru.split', 'w', encoding='utf-8') as out_f:\n",
    "    for line in news:\n",
    "        out_f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's tokenize our data and find named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Чавушоглу сказал , что РФ и раньше отрицала случай нарушения воздушного пространства Турции , когда был сбит истребитель Су-24 , и добавил , но последнее нарушение является зафиксированным фактом .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('раньше', 'ADV')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     /home/universome/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/universome/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/universome/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_ru')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def replace_nes(sentence):\n",
    "    chunks = ne_chunk(pos_tag(word_tokenize(sentence), lang='rus'))\n",
    "    tokens_with_nes = [chunk_to_token(chunk) for chunk in chunks]\n",
    "    \n",
    "    return ' '.join(tokens_with_nes)\n",
    "\n",
    "\n",
    "def chunk_to_token(chunk):\n",
    "    # This is the only way to detect NE of nltk result I'v found :|\n",
    "    if hasattr(chunk, 'label'):\n",
    "        return '__NE_' + chunk.label() + '__'\n",
    "    else:\n",
    "        return chunk[0]\n",
    "\n",
    "\n",
    "# with mp.Pool(processes=num_threads) as pool:\n",
    "#     # Cant use tqdm because it crashes the page\n",
    "#     # classics = list(tqdm(pool.imap(replace_nes, classics), total=len(classics)))\n",
    "#     # news = list(tqdm(pool.imap(replace_nes, news), total=len(news)))\n",
    "#     classics = pool.map(replace_nes, classics)\n",
    "#     news = pool.map(replace_nes, news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/generated/classics.ner.tok', 'w', encoding='utf-8') as out_f:\n",
    "    for line in classics:\n",
    "        out_f.write(line + '\\n')\n",
    "        \n",
    "with open('../data/generated/news.ru.ner.tok', 'w', encoding='utf-8') as out_f:\n",
    "    for line in news:\n",
    "        out_f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see, how much tokens we share in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of news vocabulary 1347682\n",
      "Size of classics vocabulary 365282\n",
      "How much tokens intersect? 187391\n"
     ]
    }
   ],
   "source": [
    "from src.vocab import Vocab\n",
    "\n",
    "news = open('../data/generated/news.ru.ner.tok', encoding='utf-8').read().splitlines()\n",
    "classics = open('../data/generated/classics.ner.tok', encoding='utf-8').read().splitlines()\n",
    "\n",
    "vocab_news = Vocab.from_sequences(news)\n",
    "vocab_classics = Vocab.from_sequences(classics)\n",
    "\n",
    "vocab_news = set(vocab_news.token2id.keys())\n",
    "vocab_classics = set(vocab_classics.token2id.keys())\n",
    "\n",
    "print('Size of news vocabulary', len(vocab_news))\n",
    "print('Size of classics vocabulary', len(vocab_classics))\n",
    "print('How much tokens intersect?', len(vocab_news.intersection(vocab_classics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can learn and apply BPEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "subword_nmt=\"../ext-libs/subword-nmt\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "data_src=\"$generated_data_dir/news.ru.ner.tok\"\n",
    "data_trg=\"$generated_data_dir/classics.ner.tok\"\n",
    "\n",
    "# We purposely set such low amount of BPEs\n",
    "# so our model is more like char-rnn\n",
    "num_bpes=1000\n",
    "\n",
    "bpes=\"$generated_data_dir/news-classics.bpes\"\n",
    "vocab_src=\"$generated_data_dir/classics.vocab\"\n",
    "vocab_trg=\"$generated_data_dir/news.ru.vocab\"\n",
    "\n",
    "# Learning BPEs\n",
    "python \"$subword_nmt/learn_joint_bpe_and_vocab.py\" --input $data_src $data_trg \\\n",
    "    -s $num_bpes -o $bpes --write-vocabulary $vocab_src $vocab_trg\n",
    "\n",
    "# Let's apply bpe here for our tokenized files\n",
    "python \"$subword_nmt/apply_bpe.py\" -c $bpes < $data_src > $data_src.bpe\n",
    "python \"$subword_nmt/apply_bpe.py\" -c $bpes < $data_trg > $data_trg.bpe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
