{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should preprocess Classics and News corpora:\n",
    "* split each one into individual sentences;\n",
    "* tokenize each one and replace named entities with special tokens;\n",
    "* calculate, how much dictionaries diverge (I really hope that they are almost the same)\n",
    "* learn joint BPEs\n",
    "* apply learnt BPEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../data/classics/*.txt >> ../data/generated/classics.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**. We run the following cell in shell, not in jupyter notebook, because somehow it hangs here :|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# num_threads=20\n",
    "\n",
    "# ../ext-libs/mosesdecoder/scripts/ems/support/split-sentences.perl -l ru -threads $num_threads \\\n",
    "#     < \"../data/generated/classics.txt\" > \"../data/generated/classics.split\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's tokenize now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: ru\n",
      "Number of threads: 10\n",
      "Tokenizer Version 1.1\n",
      "Language: ru\n",
      "Number of threads: 10\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mosesdecoder=\"../ext-libs/mosesdecoder\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "\n",
    "threads=10\n",
    "\n",
    "cat \"$generated_data_dir/classics.split\" | \\\n",
    "    #$mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads -l ru > \\\n",
    "    $generated_data_dir/classics.tok\n",
    "    \n",
    "cat \"$data_dir/news/news.2016.ru.shuffled\" | \\\n",
    "    #$mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads -l ru > \\\n",
    "    $generated_data_dir/news.ru.tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's extract and replace named entities now.\n",
    "\n",
    "**DISCLAIMER.** Well, currently we cant do it because of bugs in deepmit/ner and because of low quality of nltk NER package on russian text. So, let's skip this for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ner\n",
    "# from tqdm import tqdm; tqdm.monitor_interval = 0\n",
    "\n",
    "# extractor = ner.Extractor(model_url='http://lnsigo.mipt.ru/export/models/ner/ner_model_total_rus.tar.gz')\n",
    "\n",
    "# min_len = 20\n",
    "# max_len = 150\n",
    "# classics = open('../data/generated/classics.tok', encoding='utf-8').read().splitlines()\n",
    "# news = open('../data/generated/news.ru.tok', encoding='utf-8').read().splitlines()\n",
    "# classics = [s for s in classics if min_len < len(s.split()) < max_len]\n",
    "# news = [s for s in news if min_len < len(s.split()) < max_len]\n",
    "\n",
    "# # We have an awkward bug in ner, which fails on strings like 'Ивано-вичу'\n",
    "# # https://github.com/deepmipt/ner/issues/9\n",
    "# classics = [s.replace('-вичу', 'вичу') for s in classics]\n",
    "\n",
    "# def replace_nes(corpus):\n",
    "#     for i,s in enumerate(corpus):\n",
    "#         for m in reversed(list(extractor(s))):\n",
    "#             s = s[:m.span.start] + '__NE_' + m.type + '__' + s[m.span.end:]\n",
    "\n",
    "#         corpus[i] = s\n",
    "        \n",
    "#         # tqdm hangs out the page, so let's print info manually\n",
    "#         if (i+1) % 100000 == 0:\n",
    "#             print('Steps done: {}/{}'.format(i+1, len(corpus)))\n",
    "\n",
    "# replace_nes(classics)\n",
    "# replace_nes(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/generated/classics.ner', 'w', encoding='utf-8') as out_f:\n",
    "#     for line in classics:\n",
    "#         out_f.write(line + '\\n')\n",
    "        \n",
    "# with open('../data/generated/news.ru.ner', 'w', encoding='utf-8') as out_f:\n",
    "#     for line in news:\n",
    "#         out_f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see, how much tokens we share in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of news vocabulary 1326568\n",
      "Size of classics vocabulary 871402\n",
      "How much tokens intersect? 374130\n"
     ]
    }
   ],
   "source": [
    "from src.vocab import Vocab\n",
    "\n",
    "news = open('../data/generated/news.ru.tok', encoding='utf-8').read().splitlines()\n",
    "classics = open('../data/generated/classics.tok', encoding='utf-8').read().splitlines()\n",
    "\n",
    "vocab_news = Vocab.from_sequences(news)\n",
    "vocab_classics = Vocab.from_sequences(classics)\n",
    "\n",
    "vocab_news = set(vocab_news.token2id.keys())\n",
    "vocab_classics = set(vocab_classics.token2id.keys())\n",
    "\n",
    "print('Size of news vocabulary', len(vocab_news))\n",
    "print('Size of classics vocabulary', len(vocab_classics))\n",
    "print('How much tokens intersect?', len(vocab_news.intersection(vocab_classics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can learn and apply BPEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "subword_nmt=\"../ext-libs/subword-nmt\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "data_src=\"$generated_data_dir/news.ru.tok\"\n",
    "data_trg=\"$generated_data_dir/classics.tok\"\n",
    "\n",
    "# We purposely set such low amount of BPEs\n",
    "# so our model is more like char-rnn\n",
    "num_bpes=1000\n",
    "\n",
    "bpes=\"$generated_data_dir/news-classics.bpes\"\n",
    "vocab_src=\"$generated_data_dir/classics.vocab\"\n",
    "vocab_trg=\"$generated_data_dir/news.ru.vocab\"\n",
    "\n",
    "# Learning BPEs\n",
    "python \"$subword_nmt/learn_joint_bpe_and_vocab.py\" --input $data_src $data_trg \\\n",
    "    -s $num_bpes -o $bpes --write-vocabulary $vocab_src $vocab_trg\n",
    "\n",
    "# Let's apply bpe here for our tokenized files\n",
    "python \"$subword_nmt/apply_bpe.py\" -c $bpes < $data_src > $data_src.bpe\n",
    "python \"$subword_nmt/apply_bpe.py\" -c $bpes < $data_trg > $data_trg.bpe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
