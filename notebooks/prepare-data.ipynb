{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should preprocess Dostoevsky and News corpora:\n",
    "* split each one into individual sentences;\n",
    "* tokenize each one;\n",
    "* calculate, how much dictionaries diverge (I really hope that they are almost the same)\n",
    "* learn joint BPEs\n",
    "* apply BPE\n",
    "* learn strong embeddings for these BPEs on a joint shuffled corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split sentences in Dostoevsky (there can be very long sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence Splitter v3\n",
      "Language: ru\n"
     ]
    }
   ],
   "source": [
    "# DISCLAIMER\n",
    "# We run this command in shell, not in jupyter notebook,\n",
    "# because it does not show intermediate outputs\n",
    "\n",
    "# %%bash\n",
    "\n",
    "# mosesdecoder=\"../ext-libs/mosesdecoder\"\n",
    "# mkdir -p \"../data/generated/classic-books\"\n",
    "\n",
    "# for file in $(ls ../data/classic-books); do\n",
    "\n",
    "# echo \"$file -> ${file::-4}.split.txt\" && \\\n",
    "# $mosesdecoder/scripts/ems/support/split-sentences.perl -l ru -threads 20 \\\n",
    "#     < ../data/classic-books/$file > ../data/generated/classic-books/${file::-4}.split.txt\n",
    "\n",
    "# done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../data/generated/classic-books/*.txt >> ../data/generated/classics.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: ru\n",
      "Number of threads: 20\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mosesdecoder=\"../ext-libs/mosesdecoder\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "\n",
    "threads=20\n",
    "\n",
    "cat \"$generated_data_dir/classics.txt\" | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads -l ru > \\\n",
    "    $generated_data_dir/classics.tok\n",
    "    \n",
    "cat \"$data_dir/news.2016.ru.shuffled\" | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads -l ru > \\\n",
    "    $generated_data_dir/news.ru.tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see, how much tokens we share in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of news vocabulary 1326362\n",
      "Size of classics vocabulary 778191\n",
      "How much tokens intersect? 360441\n"
     ]
    }
   ],
   "source": [
    "from src.vocab import Vocab\n",
    "\n",
    "news = open('../data/generated/news.ru.tok', encoding='utf-8').read().splitlines()\n",
    "classics = open('../data/generated/classics.tok', encoding='utf-8').read().splitlines()\n",
    "\n",
    "vocab_news = Vocab.from_sequences(news)\n",
    "vocab_classics = Vocab.from_sequences(classics)\n",
    "\n",
    "vocab_news = set(vocab_news.token2id.keys())\n",
    "vocab_classics = set(vocab_classics.token2id.keys())\n",
    "\n",
    "print('Size of news vocabulary', len(vocab_news))\n",
    "print('Size of classics vocabulary', len(vocab_classics))\n",
    "print('How much tokens intersect?', len(vocab_news.intersection(vocab_classics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can learn and apply BPEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "subword_nmt=\"../ext-libs/subword-nmt\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "data_src=\"$generated_data_dir/news.ru.tok\"\n",
    "data_trg=\"$generated_data_dir/classics.tok\"\n",
    "num_bpes_src=4000\n",
    "num_bpes_trg=4000\n",
    "\n",
    "bpes_src=\"$generated_data_dir/news.ru.bpes\"\n",
    "bpes_trg=\"$generated_data_dir/classics.bpes\"\n",
    "\n",
    "python \"$subword_nmt/learn_bpe.py\" -s $num_bpes_src < $data_src > $bpes_src\n",
    "python \"$subword_nmt/learn_bpe.py\" -s $num_bpes_trg < $data_trg > $bpes_trg\n",
    "\n",
    "# Let's apply bpe here for our tokenized files\n",
    "python \"$subword_nmt/apply_bpe.py\" -c $bpes_src < $data_src > $data_src.bpe\n",
    "python \"$subword_nmt/apply_bpe.py\" -c $bpes_trg < $data_trg > $data_trg.bpe\n",
    "\n",
    "vocab_src=\"$generated_data_dir/news.ru.vocab\"\n",
    "vocab_trg=\"$generated_data_dir/classics.vocab\"\n",
    "\n",
    "# And finally, we should generate vocab\n",
    "python \"$subword_nmt/get_vocab.py\" < $bpes_src > $vocab_src\n",
    "python \"$subword_nmt/get_vocab.py\" < $bpes_trg > $vocab_trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, actually it's not very correct to learn embeddings this way, because we have 7m (1.4G) lines of News corpora and only 100k (20mb) lines of Dostoevsky. But, let's try to do it this way anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "num_threads = 20\n",
    "dim = 512\n",
    "\n",
    "model_src = fasttext.skipgram('../data/generated/news.ru.tok.bpe',\n",
    "                              '../trained_models/news.ru.tok.bpe.skipgram',\n",
    "                              dim=dim, min_count=1, silent=0, thread=num_threads)\n",
    "\n",
    "model_trg = fasttext.skipgram('../data/generated/classics.tok.bpe',\n",
    "                              '../trained_models/classics.tok.bpe.skipgram',\n",
    "                              dim=dim, min_count=1, silent=0, thread=num_threads)\n",
    "\n",
    "model_trg_tok = fasttext.skipgram('../data/generated/classics.tok',\n",
    "                                  '../trained_models/classics.tok.skipgram',\n",
    "                                  dim=dim, min_count=5, silent=0, thread=num_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
