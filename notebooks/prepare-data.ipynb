{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should preprocess Dostoevsky and News corpora:\n",
    "* split each one into individual sentences;\n",
    "* tokenize each one;\n",
    "* calculate, how much dictionaries diverge (I really hope that they are almost the same)\n",
    "* learn joint BPEs\n",
    "* apply BPE\n",
    "* learn strong embeddings for these BPEs on a joint shuffled corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split sentences in Dostoevsky (there can be very long sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence Splitter v3\n",
      "Language: ru\n"
     ]
    }
   ],
   "source": [
    "# DISCLAIMER\n",
    "# We run this command in shell, not in jupyter notebook,\n",
    "# because it does not show intermediate outputs\n",
    "\n",
    "# %%bash\n",
    "\n",
    "# mosesdecoder=\"../ext-libs/mosesdecoder\"\n",
    "# mkdir -p \"../data/generated/classics\"\n",
    "\n",
    "# for file in $(ls ../data/classics); do\n",
    "# echo \"$file -> ${file::-4}.split.txt\" && \\\n",
    "# $mosesdecoder/scripts/ems/support/split-sentences.perl -l ru -threads 20 \\\n",
    "#     < \"../data/classics/$file\" > \"../data/generated/classics/${file::-4}.split.txt\"\n",
    "# done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../data/generated/classics/*.txt >> ../data/generated/classics.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: ru\n",
      "Number of threads: 20\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mosesdecoder=\"../ext-libs/mosesdecoder\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "\n",
    "threads=20\n",
    "\n",
    "cat \"$generated_data_dir/classics.txt\" | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads -l ru > \\\n",
    "    $generated_data_dir/classics.tok\n",
    "    \n",
    "cat \"$data_dir/news.2016.ru.shuffled\" | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads -l ru > \\\n",
    "    $generated_data_dir/news.ru.tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see, how much tokens we share in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of news vocabulary 1326362\n",
      "Size of classics vocabulary 873226\n",
      "How much tokens intersect? 374421\n"
     ]
    }
   ],
   "source": [
    "from src.vocab import Vocab\n",
    "\n",
    "news = open('../data/generated/news.ru.tok', encoding='utf-8').read().splitlines()\n",
    "classics = open('../data/generated/classics.tok', encoding='utf-8').read().splitlines()\n",
    "\n",
    "vocab_news = Vocab.from_sequences(news)\n",
    "vocab_classics = Vocab.from_sequences(classics)\n",
    "\n",
    "vocab_news = set(vocab_news.token2id.keys())\n",
    "vocab_classics = set(vocab_classics.token2id.keys())\n",
    "\n",
    "print('Size of news vocabulary', len(vocab_news))\n",
    "print('Size of classics vocabulary', len(vocab_classics))\n",
    "print('How much tokens intersect?', len(vocab_news.intersection(vocab_classics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can learn and apply BPEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "subword_nmt=\"../ext-libs/subword-nmt\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "data_src=\"$generated_data_dir/news.ru.tok\"\n",
    "data_trg=\"$generated_data_dir/classics.tok\"\n",
    "\n",
    "# We purposely set such low amount of BPEs\n",
    "# so our model is more like char-rnn\n",
    "num_bpes=1000\n",
    "\n",
    "bpes=\"$generated_data_dir/news-classics.bpes\"\n",
    "vocab_src=\"$generated_data_dir/classics.vocab\"\n",
    "vocab_trg=\"$generated_data_dir/news.ru.vocab\"\n",
    "\n",
    "# Learning BPEs\n",
    "python \"$subword_nmt/learn_joint_bpe_and_vocab.py\" --input $data_src $data_trg \\\n",
    "    -s $num_bpes -o $bpes --write-vocabulary $vocab_src $vocab_trg\n",
    "\n",
    "# Let's apply bpe here for our tokenized files\n",
    "python \"$subword_nmt/apply_bpe.py\" -c $bpes < $data_src > $data_src.bpe\n",
    "python \"$subword_nmt/apply_bpe.py\" -c $bpes < $data_trg > $data_trg.bpe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
