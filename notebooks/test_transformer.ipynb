{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should make sure that our Transformer implementation works properly. For this purpose we gonna learn to translate on En-De pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize our sentences (TODO: should we use `moses` for this and not `nltk` because moses can normalize punctuation?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# cat ../data/multi30k/val.en | \\\n",
    "#     ../ext-libs/mosesdecoder/scripts/tokenizer/normalize-punctuation.perl | \\\n",
    "#     ../ext-libs/mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 1 > \\\n",
    "#     ../data/generated/kek\n",
    "    \n",
    "# head ../data/generated/kek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_data_dir = '../data/multi30k'\n",
    "output_data_dir = '../data/generated'\n",
    "\n",
    "if not os.path.exists(output_data_dir): os.mkdir(output_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/universome/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "for file_name in os.listdir(input_data_dir):\n",
    "    input_file_path = '{}/{}'.format(input_data_dir, file_name)\n",
    "    output_file_path = '{}/{}.tok'.format(output_data_dir, file_name)\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    \n",
    "    tokenized = [' '.join(nltk.word_tokenize(line)) for line in lines]\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for line in tokenized:\n",
    "            file.write(line + os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating BPEs (joint for both langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "src=\"../data/generated/train.en.tok\"\n",
    "trg=\"../data/generated/train.de.tok\"\n",
    "num_bpes=16000\n",
    "\n",
    "bpes=\"../data/generated/bpes\"\n",
    "src_vocab=\"../data/generated/vocab.en\"\n",
    "trg_vocab=\"../data/generated/vocab.de\"\n",
    "\n",
    "python ../ext-libs/subword-nmt/learn_joint_bpe_and_vocab.py \\\n",
    "    --input \"$src\" \"$trg\" \\\n",
    "    -s \"$num_bpes\" \\\n",
    "    -o \"$bpes\" \\\n",
    "    --write-vocabulary \"$src_vocab\" \"$trg_vocab\"\n",
    "\n",
    "# Now we should apply bpe for all our train/test/val files\n",
    "for file in $(ls ../data/multi30k)\n",
    "do\n",
    "    vocab=\"../data/generated/vocab.${file: -2}\"\n",
    "  \n",
    "    python ../ext-libs/subword-nmt/apply_bpe.py -c $bpes \\\n",
    "       --vocabulary \"$vocab\" \\\n",
    "       < \"../data/multi30k/$file\" > \"../data/generated/$file.bpe\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have nice dataset, which we can use for training.\n",
    "So let's train our Transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
