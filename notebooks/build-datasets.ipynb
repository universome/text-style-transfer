{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn embeddings from WMT and will learn translation task from multi30k, so results are more comparable (if we would extract 30k sentences from WMT, we couldn't compare with anybody). Besides, in the article authors do precisely this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing: tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mosesdecoder=\"../ext-libs/mosesdecoder\"\n",
    "multi30k_data_dir=\"../data/multi30k\"\n",
    "europarl_data_dir=\"../data/europarl-v7\"\n",
    "generated_data_dir=\"../data/generated\"\n",
    "\n",
    "threads=6\n",
    "\n",
    "mkdir -p generated_data_dir\n",
    "\n",
    "# Tokenizing multi30k\n",
    "for file in $(ls \"$multi30k_data_dir\")\n",
    "do\n",
    "    cat \"$multi30k_data_dir/$file\" | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads > \\\n",
    "    $generated_data_dir/$file.tok\n",
    "done\n",
    "\n",
    "for file in $(ls \"$europarl_data_dir\")\n",
    "do\n",
    "    lang=\"${file: -2}\"\n",
    "    cat \"$multi30k_data_dir/$file\" | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads > \\\n",
    "    $generated_data_dir/europarl.$lang.tok\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have tokenized staff. Let's now compute BPEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "subword_nmt=\"../ext-libs/subword-nmt\"\n",
    "src=\"../data/generated/europarl.en.tok\"\n",
    "trg=\"../data/generated/europarl.fr.tok\"\n",
    "# num_bpes=8000\n",
    "num_bpes_src=8000\n",
    "num_bpes_trg=8000\n",
    "\n",
    "# bpes=\"../data/generated/bpes\"\n",
    "bpes_src=\"../data/generated/bpes.en\"\n",
    "bpes_trg=\"../data/generated/bpes.fr\"\n",
    "vocab_src=\"../data/generated/vocab.en\"\n",
    "vocab_trg=\"../data/generated/vocab.fr\"\n",
    "\n",
    "# python ../ext-libs/subword-nmt/learn_joint_bpe_and_vocab.py \\\n",
    "#     --input \"$src\" \"$trg\" \\\n",
    "#     -s \"$num_bpes\" \\\n",
    "#     -o \"$bpes\" \\\n",
    "#     --write-vocabulary \"$vocab_src\" \"$vocab_trg\"\n",
    "python \"$subword_nmt/learn_bpe.py\" -s $num_bpes_src < $src > $bpes_src\n",
    "python \"$subword_nmt/learn_bpe.py\" -s $num_bpes_trg < $trg > $bpes_trg\n",
    "\n",
    "# Let's apply bpe here for all our tokenized files\n",
    "for file in $(ls ../data/generated/*.tok)\n",
    "do\n",
    "    lang=\"${file: -6:2}\"\n",
    "    python \"$subword_nmt/apply_bpe.py\" -c \"../data/generated/bpes.$lang\" < \"$file\" > \"$file.bpe\"\n",
    "done\n",
    "\n",
    "# And finally, we should generate vocab\n",
    "python \"$subword_nmt/get_vocab.py\" < \"$src.bpe\" > $vocab_src\n",
    "python \"$subword_nmt/get_vocab.py\" < \"$trg.bpe\" > $vocab_trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not a good thing to learn embeddings here, buut..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "num_threads = 6\n",
    "\n",
    "model_src = fasttext.skipgram('../data/generated/europarl.en.tok.bpe',\n",
    "                              '../trained_models/europarl.en.tok.bpe_cbow',\n",
    "                              dim=512, min_count=1, silent=0, thread=num_threads)\n",
    "\n",
    "model_trg = fasttext.skipgram('../data/generated/europarl.fr.tok.bpe',\n",
    "                              '../trained_models/europarl.fr.tok.bpe_cbow',\n",
    "                              dim=512, min_count=1, silent=0, thread=num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove .bin files which we do not use\n",
    "!rm ../trained_models/*.bin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
