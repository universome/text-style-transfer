{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn embeddings from WMT and will learn translation task from multi30k, so results are more comparable (if we would extract 30k sentences from WMT, we couldn't compare with anybody). Besides, in the article authors do precisely this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing: tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "multi30k_data_dir = '../data/multi30k'\n",
    "wmt17_data_dir = '../data/wmt17'\n",
    "generated_data_dir = '../data/generated'\n",
    "\n",
    "if not os.path.exists(generated_data_dir): os.mkdir(generated_data_dir)\n",
    "\n",
    "nltk.download('punkt')\n",
    "files_to_tokenize = []\n",
    "\n",
    "# Tokenizing multi30k\n",
    "for file_name in os.listdir(multi30k_data_dir):\n",
    "    input_file_path = '{}/{}'.format(multi30k_data_dir, file_name)\n",
    "    output_file_path = '{}/{}.tok'.format(generated_data_dir, file_name)\n",
    "\n",
    "    files_to_tokenize.append((input_file_path, output_file_path))\n",
    "\n",
    "# Tokenizing WMT\n",
    "wmt17_file_name_src = '{}/{}'.format(wmt17_data_dir, 'europarl-v7.de-en.en')\n",
    "wmt17_file_name_trg = '{}/{}'.format(wmt17_data_dir, 'europarl-v7.de-en.de')\n",
    "files_to_tokenize.append((wmt17_file_name_src, '%s/wmt17.en.tok' % generated_data_dir))\n",
    "files_to_tokenize.append((wmt17_file_name_trg, '%s/wmt17.de.tok' % generated_data_dir))\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "for input_file_path, output_file_path in files_to_tokenize:\n",
    "    print('Tokenizing', input_file_path)\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    \n",
    "    tokenized = [' '.join(nltk.word_tokenize(line)) for line in lines]\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for line in tokenized:\n",
    "            file.write(line + os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have tokenized staff. Let's now compute BPEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# TODO: use WMT after dev is done\n",
    "# src=\"../data/generated/wmt17.en.tok\"\n",
    "# trg=\"../data/generated/wmt17.de.tok\"\n",
    "src=\"../data/generated/train.en.tok\"\n",
    "trg=\"../data/generated/train.de.tok\"\n",
    "num_bpes=16000\n",
    "\n",
    "bpes=\"../data/generated/bpes\"\n",
    "src_vocab=\"../data/generated/vocab.en\"\n",
    "trg_vocab=\"../data/generated/vocab.de\"\n",
    "\n",
    "python ../ext-libs/subword-nmt/learn_joint_bpe_and_vocab.py \\\n",
    "    --input \"$src\" \"$trg\" \\\n",
    "    -s \"$num_bpes\" \\\n",
    "    -o \"$bpes\" \\\n",
    "    --write-vocabulary \"$src_vocab\" \"$trg_vocab\"\n",
    "\n",
    "# Let's apply bpe here for all our tokenized files\n",
    "for file in $(ls ../data/generated/*.tok)\n",
    "do\n",
    "    lang=\"${file: -6:2}\"\n",
    "    echo \"For file $file we use lang: $lang.\"\n",
    "    vocab=\"../data/generated/vocab.$lang\"\n",
    "  \n",
    "    python ../ext-libs/subword-nmt/apply_bpe.py -c $bpes \\\n",
    "       --vocabulary \"$vocab\" < \"$file\" > \"$file.bpe\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not a good thing to learn embeddings here, buut..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "\n",
    "# TODO: skipgram is more accurate (but slower to train)\n",
    "# TODO: do not forget to use wmt17 after development is done\n",
    "# model_src = fasttext.cbow('../data/generated/wmt17.en.tok.bpe', '../trained_models/wmt17.en.tok.bpe_cbow', dim=512)\n",
    "# model_trg = fasttext.cbow('../data/generated/wmt17.de.tok.bpe', '../trained_models/wmt17.de.tok.bpe_cbow', dim=512)\n",
    "model_src = fasttext.cbow('../data/generated/train.en.tok.bpe', '../trained_models/wmt17.en.tok.bpe_cbow',\n",
    "                          dim=512, min_count=1, silent=0)\n",
    "model_trg = fasttext.cbow('../data/generated/train.de.tok.bpe', '../trained_models/wmt17.de.tok.bpe_cbow',\n",
    "                          dim=512, min_count=1, silent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove .bin files which we do not use\n",
    "!rm ../trained_models/*.bin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
