{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn embeddings from WMT and will learn translation task from multi30k, so results are more comparable (if we would extract 30k sentences from WMT, we couldn't compare with anybody). Besides, in the article authors do precisely this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing: tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/universome/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Tokenizing ../data/multi30k/train.de\n",
      "Tokenizing ../data/multi30k/train.en\n",
      "Tokenizing ../data/multi30k/val.en\n",
      "Tokenizing ../data/multi30k/test.en\n",
      "Tokenizing ../data/multi30k/test.de\n",
      "Tokenizing ../data/multi30k/val.de\n",
      "Tokenizing ../data/wmt17/europarl-v7.de-en.en\n",
      "Tokenizing ../data/wmt17/europarl-v7.de-en.de\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "multi30k_data_dir = '../data/multi30k'\n",
    "wmt17_data_dir = '../data/wmt17'\n",
    "generated_data_dir = '../data/generated'\n",
    "\n",
    "if not os.path.exists(generated_data_dir): os.mkdir(generated_data_dir)\n",
    "\n",
    "nltk.download('punkt')\n",
    "files_to_tokenize = []\n",
    "\n",
    "# Tokenizing multi30k\n",
    "for file_name in os.listdir(multi30k_data_dir):\n",
    "    input_file_path = '{}/{}'.format(multi30k_data_dir, file_name)\n",
    "    output_file_path = '{}/{}.tok'.format(generated_data_dir, file_name)\n",
    "\n",
    "    files_to_tokenize.append((input_file_path, output_file_path))\n",
    "\n",
    "# Tokenizing WMT\n",
    "wmt17_file_name_src = '{}/{}'.format(wmt17_data_dir, 'europarl-v7.de-en.en')\n",
    "wmt17_file_name_trg = '{}/{}'.format(wmt17_data_dir, 'europarl-v7.de-en.de')\n",
    "files_to_tokenize.append((wmt17_file_name_src, '%s/wmt17.en.tok' % generated_data_dir))\n",
    "files_to_tokenize.append((wmt17_file_name_trg, '%s/wmt17.de.tok' % generated_data_dir))\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "for input_file_path, output_file_path in files_to_tokenize:\n",
    "    print('Tokenizing', input_file_path)\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    \n",
    "    tokenized = [' '.join(nltk.word_tokenize(line)) for line in lines]\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for line in tokenized:\n",
    "            file.write(line + os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have tokenized staff. Let's now compute BPEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "subword_nmt=\"../ext-libs/subword-nmt\"\n",
    "src=\"../data/generated/wmt17.en.tok\"\n",
    "trg=\"../data/generated/wmt17.de.tok\"\n",
    "# num_bpes=8000\n",
    "num_bpes_src=4000\n",
    "num_bpes_trg=4000\n",
    "\n",
    "# bpes=\"../data/generated/bpes\"\n",
    "bpes_src=\"../data/generated/bpes.en\"\n",
    "bpes_trg=\"../data/generated/bpes.de\"\n",
    "vocab_src=\"../data/generated/vocab.en\"\n",
    "vocab_trg=\"../data/generated/vocab.de\"\n",
    "\n",
    "# python ../ext-libs/subword-nmt/learn_joint_bpe_and_vocab.py \\\n",
    "#     --input \"$src\" \"$trg\" \\\n",
    "#     -s \"$num_bpes\" \\\n",
    "#     -o \"$bpes\" \\\n",
    "#     --write-vocabulary \"$vocab_src\" \"$vocab_trg\"\n",
    "python \"$subword_nmt/learn_bpe.py\" -s $num_bpes_src < $src > $bpes_src\n",
    "python \"$subword_nmt/learn_bpe.py\" -s $num_bpes_trg < $trg > $bpes_trg\n",
    "\n",
    "# Let's apply bpe here for all our tokenized files\n",
    "for file in $(ls ../data/generated/*.tok)\n",
    "do\n",
    "    lang=\"${file: -6:2}\"\n",
    "    python \"$subword_nmt/apply_bpe.py\" -c \"../data/generated/bpes.$lang\" < \"$file\" > \"$file.bpe\"\n",
    "done\n",
    "\n",
    "# And finally, we should generate vocab\n",
    "python \"$subword_nmt/get_vocab.py\" < \"$src.bpe\" > $vocab_src\n",
    "python \"$subword_nmt/get_vocab.py\" < \"$trg.bpe\" > $vocab_trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not a good thing to learn embeddings here, buut..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "\n",
    "model_src = fasttext.skipgram('../data/generated/wmt17.en.tok.bpe',\n",
    "                              '../trained_models/wmt17.en.tok.bpe_cbow',\n",
    "                              dim=512, min_count=1, silent=0, thread=4)\n",
    "\n",
    "model_trg = fasttext.skipgram('../data/generated/wmt17.de.tok.bpe',\n",
    "                              '../trained_models/wmt17.de.tok.bpe_cbow',\n",
    "                              dim=512, min_count=1, silent=0, thread=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove .bin files which we do not use\n",
    "# !rm ../trained_models/*.bin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
