{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "% env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import islice\n",
    "\n",
    "from src.vocab import Vocab\n",
    "\n",
    "DATA_PATH = '../data/generated'\n",
    "\n",
    "# We set rather large min_len, because our BPEs are very short\n",
    "min_len = 50\n",
    "max_len = 250\n",
    "\n",
    "classics_path = os.path.join(DATA_PATH, 'classics.ne.split.tok.bpe')\n",
    "news_path = os.path.join(DATA_PATH, 'news.ru.tok.ne.bpe')\n",
    "\n",
    "classics = open(classics_path, 'r', encoding='utf-8').read().splitlines()\n",
    "news = open(news_path, 'r', encoding='utf-8').read().splitlines()\n",
    "# TODO: replace with full file when dev is done\n",
    "# with open(classics_path, encoding='utf-8') as classics_file, open(news_path, encoding='utf-8') as news_file:\n",
    "#     classics = list(islice(classics_file, 10**4))\n",
    "#     news = list(islice(news_file, 10**4))\n",
    "\n",
    "classics = [s for s in classics if min_len < len(s.split()) < (max_len - 2)]\n",
    "news = [s for s in news if min_len < len(s.split()) < (max_len - 2)]\n",
    "\n",
    "# There are sentences which do not contain a single normal\n",
    "# (i.e. fully alphabetic) word. Let's remove them.\n",
    "classics = [s for s in classics if any([w.isalpha() for w in s.replace('@@ ', '').split()])]\n",
    "news = [s for s in news if any([w.isalpha() for w in s.replace('@@ ', '').split()])]\n",
    "\n",
    "vocab = Vocab.from_sequences(classics + news)\n",
    "vocab.token2id['__DROP__'] = len(vocab)\n",
    "vocab.tokens.append('__DROP__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "np.random.shuffle(classics)\n",
    "np.random.shuffle(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classics, classics_val = classics[:-1000], classics[-1000:]\n",
    "news, news_val = news[:-1000], news[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "\n",
    "from src.models import Transformer\n",
    "from src.dataloaders import WordRecoveryDataloader, OneSidedDataloader\n",
    "from src.vocab import constants\n",
    "from src.trainers import WordRecoveryTrainer\n",
    "\n",
    "log_file_path = '../logs/word_recovery.log'\n",
    "\n",
    "# Let's clean log file\n",
    "if os.path.exists(log_file_path): os.remove(log_file_path)\n",
    "\n",
    "def reconstruction_criterion(vocab_size):\n",
    "    ''' With PAD token zero weight '''\n",
    "    weight = torch.ones(vocab_size)\n",
    "    weight[constants.PAD] = 0\n",
    "\n",
    "    return nn.CrossEntropyLoss(weight)\n",
    "\n",
    "transformer = Transformer(len(vocab), len(vocab), max_len)\n",
    "transformer.load_state_dict(torch.load('../trained_models/word_recovery.pth'))\n",
    "\n",
    "optimizer = Adam(transformer.get_trainable_parameters(), lr=1e-5)\n",
    "criterion = reconstruction_criterion(len(vocab))\n",
    "\n",
    "config = {\n",
    "    'max_num_epochs': 5,\n",
    "    'plot_every': 50,\n",
    "    'validate_every': 200,\n",
    "    'mixing_scheme': (0.01, 0, 100000),\n",
    "    'log_file': log_file_path,\n",
    "    'max_len': 20\n",
    "}\n",
    "\n",
    "trainer = WordRecoveryTrainer(transformer, optimizer, criterion, vocab, config)\n",
    "training_data = WordRecoveryDataloader(classics, news, vocab, trainer.mixing_coef, batch_size=32, shuffle=True)\n",
    "val_data = WordRecoveryDataloader(classics_val[:512], news_val[:512], vocab, batch_size=512)\n",
    "\n",
    "# Making validation data deterministic\n",
    "val_data = np.array(next(val_data)).transpose().tolist()\n",
    "val_data = OneSidedDataloader(val_data, batch_size=32, unpack=True, pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run_training(training_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), '../trained_models/word_recovery.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's now try to translate some news.\n",
    "For this we should generate from a sentence (of $n$ words) $n$ sentences with `__DROP__` word in place of each word.\n",
    "Named entities, I suppose, should be transferred as is. But this will be the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  В больницу , по словам заместителя главы администрации - начальника департамента информационной политики __NE_PER__ , Городецкий попал в больницу с простудой .\n",
      "Result:  В частности , по итогам американской французской Андрэ - деятельность французской французской деятельности __NE_PER__ , Андрэ вступил в соединение с американским .\n",
      "\n",
      "Source:  __NE_PER__ отметил : __NE_ORG__; __NE_LOC__ полвека накапливал радиоактивные отходы в виде отработанного топлива , выведенного из эксплуатации атомного флота , и теперь их очищают и кондиционируют .\n",
      "Result:  __NE_PER__ говорит : __NE_ORG__; __NE_LOC__ не выполняет все законы в пользу и закона , вынужденного в закона и выборов , которые поскольку они покинули и потерпеют .\n",
      "\n",
      "Source:  Реализация всех этапов развития железнодорожной инфраструктуры рассчитана до 2018 года , а общий объем капвложений оценивается более чем в 6,7 млрд руб. Ожидаемый объем перевозок на данном направлении после завершения строительства моста и реконструкции участка к 2020 году может составить 5,2 млн т , к 2025 году - 7,7 млн т.\n",
      "Result:  Из и и районе развития группы доходит выборах 2018 года , а в потребитель не и более чем более 6,7 млн руб. млрд и выпущенной в неизмеримом заседании и заместителей и продукции в и и в 2020 млн может выйти 5,2 млрд млн , в 2025 января - 7,7 млн т.\n",
      "\n",
      "Source:  В то же время , как считает __NE_PER__ , РСЧС должна постепенно отходить от принципа оперативного реагирования к принципу профилактики - предупреждению чрезвычайных ситуаций и управлению рисками .\n",
      "Result:  В то же время , как сообщил __NE_PER__ , которая будет была отказаться от уровень и отношения по закону покупателя - потребление развития закон по с пожара .\n",
      "\n",
      "Source:  Сообщается , что по плану , __NE_PER__ , __NE_PER__ и Корниенко должны были сами выбраться из капсулы , имитирую посадку на __NE_LOC__ , но им не удалось этого сделать .\n",
      "Result:  Установленные , что по справке , __NE_PER__ , __NE_PER__ и Анны должны были бы выйти в деревни , поставить напасть в __NE_LOC__ , и ему не удалось и понятия .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataloaders.word_recovery_dataloader import drop_each_word, group_bpes_into_words\n",
    "from src.utils.data_utils import token_ids_to_sents, pad_to_longest\n",
    "from src.vocab import constants\n",
    "\n",
    "news_test = news_val[15:20]\n",
    "news_test = [s for s in news_test if len(s.split()) < max_len]\n",
    "results = []\n",
    "\n",
    "transformer.eval()\n",
    "all_beams = []\n",
    "for sentence in tqdm(news_test):\n",
    "    seqs, _, words_idx = drop_each_word(sentence)\n",
    "    seqs_idx = [[vocab.token2id[t] for t in s.split()] for s in seqs]\n",
    "    seqs_idx = [[constants.BOS] + s + [constants.EOS] for s in seqs_idx]\n",
    "    seqs_idx = pad_to_longest(seqs_idx, volatile=True)\n",
    "    predictions, beams = transformer.translate_batch(seqs_idx, beam_size=1, max_len=20, return_beams=True)\n",
    "    predictions = token_ids_to_sents(predictions, vocab)\n",
    "    \n",
    "    # Now we should construct initial sentence with some care,\n",
    "    # because not each word was dropped, but only normal ones (fully alphabetical)\n",
    "    words = group_bpes_into_words(sentence)\n",
    "    for i, w in enumerate(predictions): words[words_idx[i]] = w\n",
    "    sentence = ' '.join(words)\n",
    "\n",
    "    results.append(sentence)\n",
    "    all_beams.append(beams)\n",
    "\n",
    "for i in range(len(news_test)):\n",
    "    print('Source: ', news_test[i].replace('@@ ', ''))\n",
    "    print('Result: ', results[i].replace('@@ ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentence, return_scores=False, beam_size=1):\n",
    "    seqs, _, words_idx = drop_each_word(sentence)\n",
    "    seqs_idx = [[vocab.token2id[t] for t in s.split()] for s in seqs]\n",
    "    seqs_idx = [[constants.BOS] + s + [constants.EOS] for s in seqs_idx]\n",
    "    seqs_idx = pad_to_longest(seqs_idx, volatile=True)\n",
    "    predictions, beams = transformer.translate_batch(seqs_idx, beam_size=beam_size, max_len=20, return_beams=True)\n",
    "    words_predictions = token_ids_to_sents(predictions, vocab)\n",
    "    \n",
    "    # Now we should construct initial sentence with some care,\n",
    "    # because not each word was dropped, but only normal ones (fully alphabetical)\n",
    "    words = group_bpes_into_words(sentence)\n",
    "    for i, w in enumerate(words_predictions): words[words_idx[i]] = w\n",
    "    translation = ' '.join(words)\n",
    "    \n",
    "    score = lambda i: (beams[words_idx.index(i)].scores.max() / len(beams[words_idx.index(i)].all_scores))\n",
    "    scores = [(normal_word_idx, word_idx, score(word_idx)) for normal_word_idx, word_idx in enumerate(words_idx)]\n",
    "\n",
    "    if return_scores:\n",
    "        return translation, predictions, scores\n",
    "    else:\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloaders.word_recovery_dataloader import get_words_idx\n",
    "\n",
    "def sequential_inference(sentence):\n",
    "    \"\"\"\n",
    "    Here we gonna make a more complex inference: we gonna\n",
    "    sequentially choose and replace best words\n",
    "    \"\"\"\n",
    "    translated_words_idx = set()\n",
    "    words = group_bpes_into_words(sentence)\n",
    "    num_normal_words = len(get_words_idx(sentence.replace('@@ ', '').split()))\n",
    "    \n",
    "    for _ in range(num_normal_words):\n",
    "        #print(words)\n",
    "        _, predictions, scores = inference(' '.join(words), return_scores=True)\n",
    "\n",
    "        # Now we should choose which word to translate\n",
    "        scores.sort(key=lambda x:x[1], reverse=True)\n",
    "        normal_word_idx, word_idx, score = next(s for s in scores if not s[1] in translated_words_idx)\n",
    "        #print('\\n', vocab.detokenize(predictions[word_idx][:-1]), '\\n')\n",
    "        #print(word_idx, scores)\n",
    "        words[word_idx] = vocab.detokenize(predictions[normal_word_idx][:-1])\n",
    "        translated_words_idx.add(word_idx)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:02<00:10,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source В больницу , по словам заместителя главы администрации - начальника департамента информационной политики _ _ NE _ PER _ _ , Городецкий попал в больницу с простудой .\n",
      "Result: В комнату , в поводу которой графа Андрэ - Анны и Маргариты и _ _ NE _ PER _ _ , который поступил на постели и постелью .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:06<00:09,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source _ _ NE _ PER _ _ отметил : __NE_ORG__; _ _ NE _ LOC _ _ полвека накапливал радиоактивные отходы в виде отработанного топлива , выведенного из эксплуатации атомного флота , и теперь их очищают и кондиционируют .\n",
      "Result: _ _ NE _ PER _ _ Вместо : __NE_ORG__; _ _ NE _ PER _ _ вступила в свои помещения и постели своего пола , вышел из постели и пожара , и все они покинут и покупают .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:15<00:10,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Реализация всех этапов развития железнодорожной инфраструктуры рассчитана до 2018 года , а общий объем капвложений оценивается более чем в 6,7 млрд руб. Ожидаемый объем перевозок на данном направлении после завершения строительства моста и реконструкции участка к 2020 году может составить 5,2 млн т , к 2025 году - 7,7 млн т.\n",
      "Result: Она выступала в проведении программы задачи в выборах 2018 года , а также закон выпущена и более чем более 6,7 млн руб. и продажной выставки в уровень позиции для закрытия выпущена на млрд рублей и до 2020 млн может выйти 5,2 млрд рублей , а 2025 января - 7,7 млн т.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:19<00:04,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source В то же время , как считает _ _ NE _ PER _ _ , РСЧС должна постепенно отходить от принципа оперативного реагирования к принципу профилактики - предупреждению чрезвычайных ситуаций и управлению рисками .\n",
      "Result: В то же время , как и _ _ NE _ PER _ _ , которая должна была отказаться от улицы и поездка в закону пожара - пожар и задач и закончилась поэта .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:23<00:00,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Сообщается , что по плану , _ _ NE _ PER _ _ , _ _ NE _ PER _ _ и Корниенко должны были сами выбраться из капсулы , имитирую посадку на _ _ NE _ LOC _ _ , но им не удалось этого сделать .\n",
      "Result: Все , что по приказу , _ _ NE _ PER _ _ , _ _ NE _ PER _ _ и Василька должны были были выйти из Вены , а Царя и _ _ NE _ LOC _ _ , где его не дали и понять .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm(news_val[15:20]):\n",
    "    #sentence = sentence.replace('_ _ NE _ PER _ _', '__NE_PER__')\n",
    "    sentence = sentence.replace('__NE_PER__', '_ _ NE _ PER _ _')\n",
    "    sentence = sentence.replace('__NE_LOC__', '_ _ NE _ LOC _ _')\n",
    "    translation = ' '.join(sequential_inference(sentence))\n",
    "    print('Source', sentence.replace('@@ ', ''))\n",
    "    print('Result:', translation.replace('@@ ', ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
