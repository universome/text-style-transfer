{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to train in unsupervised manner?\n",
    "For this we have to do the following:\n",
    "* add noise to input sentences (swaps and tokens removal)\n",
    "* learning word embeddings from something\n",
    "* backtranslation\n",
    "* adversarial losses:\n",
    "    * FFN (TODO: try RNN/Transformer?) discriminator(s) to distinguish encodings\n",
    "    * TODO: discriminator(s) to distinguish produced styles/translations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's now learn word embeddings.\n",
    "\n",
    "We'll learn embeddings from WMT and will learn translation task from multi30k, so results are more comparable (if we would extract 30k sentences from WMT, we couldn't compare with anybody). Besides, in the article authors do precisely this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "multi30k_data_dir = '../data/multi30k'\n",
    "wmt17_data_dir = '../data/wmt17'\n",
    "generated_data_dir = '../data/generated'\n",
    "\n",
    "if not os.path.exists(generated_data_dir): os.mkdir(generated_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing: tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/universome/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Tokenizing ../data/multi30k/train.de\n",
      "Tokenizing ../data/multi30k/train.en\n",
      "Tokenizing ../data/multi30k/val.en\n",
      "Tokenizing ../data/multi30k/test.en\n",
      "Tokenizing ../data/multi30k/test.de\n",
      "Tokenizing ../data/multi30k/val.de\n",
      "Tokenizing ../data/wmt17/europarl-v7.de-en.en\n",
      "Tokenizing ../data/wmt17/europarl-v7.de-en.de\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "files_to_tokenize = []\n",
    "\n",
    "# Tokenizing multi30k\n",
    "for file_name in os.listdir(multi30k_data_dir):\n",
    "    input_file_path = '{}/{}'.format(multi30k_data_dir, file_name)\n",
    "    output_file_path = '{}/{}.tok'.format(generated_data_dir, file_name)\n",
    "\n",
    "    files_to_tokenize.append((input_file_path, output_file_path))\n",
    "\n",
    "# Tokenizing WMT\n",
    "wmt17_file_name_src = '{}/{}'.format(wmt17_data_dir, 'europarl-v7.de-en.en')\n",
    "wmt17_file_name_trg = '{}/{}'.format(wmt17_data_dir, 'europarl-v7.de-en.de')\n",
    "files_to_tokenize.append((wmt17_file_name_src, '%s/wmt17.en.tok' % generated_data_dir))\n",
    "files_to_tokenize.append((wmt17_file_name_trg, '%s/wmt17.de.tok' % generated_data_dir))\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "for input_file_path, output_file_path in files_to_tokenize:\n",
    "    print('Tokenizing', input_file_path)\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    \n",
    "    tokenized = [' '.join(nltk.word_tokenize(line)) for line in lines]\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for line in tokenized:\n",
    "            file.write(line + os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have tokenized staff. Let's now compute BPEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For file ../data/generated/test.de.tok we use lang: de.\n",
      "For file ../data/generated/test.en.tok we use lang: en.\n",
      "For file ../data/generated/train.de.tok we use lang: de.\n",
      "For file ../data/generated/train.en.tok we use lang: en.\n",
      "For file ../data/generated/val.de.tok we use lang: de.\n",
      "For file ../data/generated/val.en.tok we use lang: en.\n",
      "For file ../data/generated/wmt17.de.tok we use lang: de.\n",
      "For file ../data/generated/wmt17.en.tok we use lang: en.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# TODO: use WMT after dev is done\n",
    "# src=\"../data/generated/wmt17.en.tok\"\n",
    "# trg=\"../data/generated/wmt17.de.tok\"\n",
    "src=\"../data/generated/train.en.tok\"\n",
    "trg=\"../data/generated/train.de.tok\"\n",
    "num_bpes=16000\n",
    "\n",
    "bpes=\"../data/generated/bpes\"\n",
    "src_vocab=\"../data/generated/vocab.en\"\n",
    "trg_vocab=\"../data/generated/vocab.de\"\n",
    "\n",
    "python ../ext-libs/subword-nmt/learn_joint_bpe_and_vocab.py \\\n",
    "    --input \"$src\" \"$trg\" \\\n",
    "    -s \"$num_bpes\" \\\n",
    "    -o \"$bpes\" \\\n",
    "    --write-vocabulary \"$src_vocab\" \"$trg_vocab\"\n",
    "\n",
    "# Let's apply bpe here for all our tokenized files\n",
    "for file in $(ls ../data/generated/*.tok)\n",
    "do\n",
    "    lang=\"${file: -6:2}\"\n",
    "    echo \"For file $file we use lang: $lang.\"\n",
    "    vocab=\"../data/generated/vocab.$lang\"\n",
    "  \n",
    "    python ../ext-libs/subword-nmt/apply_bpe.py -c $bpes \\\n",
    "       --vocabulary \"$vocab\" < \"$file\" > \"$file.bpe\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argh, we finally have BPE files for wmt17 and can generate embeddings for them. Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# TODO: skipgram is more accurate (but slower to train)\n",
    "# TODO: do not forget to use wmt17 after development is done\n",
    "# model_src = fasttext.cbow('../data/generated/wmt17.en.tok.bpe', '../trained_models/wmt17.en.tok.bpe_cbow', dim=512)\n",
    "# model_trg = fasttext.cbow('../data/generated/wmt17.de.tok.bpe', '../trained_models/wmt17.de.tok.bpe_cbow', dim=512)\n",
    "# model_src = fasttext.cbow('../data/generated/train.en.tok.bpe', '../trained_models/wmt17.en.tok.bpe_cbow',\n",
    "#                           dim=512, min_count=1, silent=0)\n",
    "# model_trg = fasttext.cbow('../data/generated/train.de.tok.bpe', '../trained_models/wmt17.de.tok.bpe_cbow',\n",
    "#                           dim=512, min_count=1, silent=0)\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    embeddings = {}\n",
    "    \n",
    "    with open(embeddings_path, 'r', encoding='utf-8') as f:\n",
    "        next(f) # Skipping first line, because it's header info\n",
    "        for line in tqdm(f):\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            embeddings[word] = np.asarray(values[1:], dtype='float32')\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def init_emb_matrix(emb_matrix, emb_dict, token2id):\n",
    "    emb_size = emb_matrix.size(1)\n",
    "    \n",
    "    for word, idx in token2id.items():\n",
    "        if not word in emb_dict:\n",
    "            print('Skipping ', word)\n",
    "            continue\n",
    "        emb_matrix[idx] = torch.FloatTensor(emb_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove .bin files which we do not use\n",
    "# !rm ../trained_models/*.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should initialize our transformer with learnt embeddings, initialize discriminator and add adversarial loss.\n",
    "When we are done with that — we are only left with training the thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7416it [00:01, 5396.73it/s]\n",
      "10502it [00:02, 4550.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping  __BOS__\n",
      "Skipping  __EOS__\n",
      "Skipping  __UNK__\n",
      "Skipping  __PAD__\n",
      "Skipping  __BOS__\n",
      "Skipping  __EOS__\n",
      "Skipping  __UNK__\n",
      "Skipping  __PAD__\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path += ['..', '../src']\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import os\n",
    "\n",
    "from src.vocab import Vocab\n",
    "from src.transformer.models import Transformer\n",
    "from src.models import FFN\n",
    "\n",
    "DATA_PATH = '../data/generated'\n",
    "max_len = 200 # TODO: Dostoevsky has much longer sentences\n",
    "\n",
    "vocab_src = Vocab.from_file(os.path.join(DATA_PATH, 'vocab.en'))\n",
    "vocab_trg = Vocab.from_file(os.path.join(DATA_PATH, 'vocab.de'))\n",
    "\n",
    "transformer = Transformer(len(vocab_src), len(vocab_trg), max_len)\n",
    "discriminator = FFN(512, 3, 1024)\n",
    "\n",
    "# Initializing transformer encoder and decoder with embeddings\n",
    "embeddings_src = load_embeddings('../trained_models/wmt17.en.tok.bpe_cbow.vec')\n",
    "embeddings_trg = load_embeddings('../trained_models/wmt17.de.tok.bpe_cbow.vec')\n",
    "\n",
    "init_emb_matrix(transformer.encoder.src_word_emb.weight.data, embeddings_src, vocab_src.token2id)\n",
    "init_emb_matrix(transformer.decoder.tgt_word_emb.weight.data, embeddings_trg, vocab_trg.token2id)\n",
    "\n",
    "train_src_path = os.path.join(DATA_PATH, 'train.en.tok.bpe')\n",
    "train_trg_path = os.path.join(DATA_PATH, 'train.de.tok.bpe')\n",
    "val_src_path = os.path.join(DATA_PATH, 'val.en.tok.bpe')\n",
    "val_trg_path = os.path.join(DATA_PATH, 'val.de.tok.bpe')\n",
    "\n",
    "train_src = open(train_src_path, 'r', encoding='utf-8').read().splitlines()\n",
    "train_trg = open(train_trg_path, 'r', encoding='utf-8').read().splitlines()\n",
    "val_src = open(val_src_path, 'r', encoding='utf-8').read().splitlines()\n",
    "val_trg = open(val_trg_path, 'r', encoding='utf-8').read().splitlines()\n",
    "\n",
    "train_src = [s.split() for s in train_src]\n",
    "train_trg = [s.split() for s in train_trg]\n",
    "val_src = [s.split() for s in val_src]\n",
    "val_trg = [s.split() for s in val_trg]\n",
    "\n",
    "train_src_idx = [[vocab_src.token2id.get(t, vocab_src.unk) for t in s] for s in train_src]\n",
    "train_trg_idx = [[vocab_trg.token2id.get(t, vocab_trg.unk) for t in s] for s in train_trg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we should write a training procedure, including backtranslation and noising.\n",
    "That's not so easy, as it may seem.\n",
    "Also we should write loss functions and add training visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.transformer.models' from '../src/transformer/models.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src\n",
    "import importlib\n",
    "\n",
    "importlib.reload(src.utils.umt_batcher)\n",
    "importlib.reload(src.transformer.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping  __BOS__\n",
      "Skipping  __EOS__\n",
      "Skipping  __UNK__\n",
      "Skipping  __PAD__\n",
      "Skipping  __BOS__\n",
      "Skipping  __EOS__\n",
      "Skipping  __UNK__\n",
      "Skipping  __PAD__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Training)   :   0%|          | 0/907 [00:00<?, ?it/s]/Users/universome/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py:325: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training discriminator\n",
      "Computing predictions\n",
      "Computing losses\n",
      "Computing gradients\n",
      "Training translator\n",
      "Computing back-translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A../src/transformer/models.py:236: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.44it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:07,  1.08it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.06s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:04<00:06,  1.07s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.11s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:06<00:04,  1.11s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:07<00:03,  1.14s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.17s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.18s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:04,  1.87it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:05,  1.38it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:02<00:05,  1.30it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:03<00:04,  1.24it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:04<00:04,  1.15it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:03,  1.02it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:07<00:03,  1.08s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.26s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:12<00:01,  1.36s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.44s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predictions (translations of back-translations)\n",
      "Computing losses\n",
      "Computing gradients\n",
      "Updating weights\n",
      "Training discriminator\n",
      "Computing predictions\n",
      "Computing losses\n",
      "Computing gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/universome/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([608])) that is different to the input size (torch.Size([608, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/Users/universome/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([544])) that is different to the input size (torch.Size([544, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating parameters\n",
      "Training generator\n",
      "Computing losses\n",
      "Computing gradients\n",
      "Updating parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  - (Training)   :   0%|          | 1/907 [01:06<16:41:19, 66.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ae_loss_src': 8.957502365112305, 'ae_loss_trg': 9.309548377990723, 'loss_bt_src': 8.96019172668457, 'loss_bt_trg': 9.308553695678711, 'discr_loss_src': 0.6752368807792664, 'discr_loss_trg': 0.7115498781204224, 'gen_loss_src': 0.7113943099975586, 'gen_loss_trg': 0.6750862002372742}\n",
      "Training discriminator\n",
      "Computing predictions\n",
      "Computing losses\n",
      "Computing gradients\n",
      "Training translator\n",
      "Computing back-translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:12,  1.44s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:16,  2.06s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:06<00:14,  2.08s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:07<00:11,  1.92s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:09<00:09,  1.86s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:10<00:06,  1.75s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:12<00:05,  1.72s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:14<00:03,  1.80s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:16<00:01,  1.89s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.58it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:06,  1.23it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:02<00:06,  1.16it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:04<00:04,  1.00it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:06<00:04,  1.11s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.27s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:11<00:02,  1.42s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:13<00:01,  1.48s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predictions (translations of back-translations)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-df7b25dfbedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Computing predictions (translations of back-translations)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mbt_src_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt_trg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_trg_embs_in_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_src_embs_in_decoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mbt_trg_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/neuro_dostoevsky/src/transformer/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, use_trg_embs_in_encoder, use_src_embs_in_decoder, return_encodings)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_src_embs_in_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_word_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mseq_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_word_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/neuro_dostoevsky/src/transformer/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt_seq, src_seq, enc_output, embs, return_attns)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mslf_attn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_slf_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 dec_enc_attn_mask=dec_enc_attn_pad_mask)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_attns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/neuro_dostoevsky/src/transformer/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dec_input, enc_output, slf_attn_mask, dec_enc_attn_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m             dec_input, dec_input, dec_input, attn_mask=slf_attn_mask)\n\u001b[1;32m     34\u001b[0m         dec_output, dec_enc_attn = self.enc_attn(\n\u001b[0;32m---> 35\u001b[0;31m             dec_output, enc_output, enc_output, attn_mask=dec_enc_attn_mask)\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/neuro_dostoevsky/src/transformer/sub_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, attn_mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# treat the result as a (n_head * mb_size) size batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mq_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_qs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n_head*mb_size) x len_q x d_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mk_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_ks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n_head*mb_size) x len_k x d_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mv_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_vs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n_head*mb_size) x len_v x d_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from src.vocab import Vocab\n",
    "from src.transformer.models import Transformer\n",
    "from src.models import FFN\n",
    "\n",
    "DATA_PATH = '../data/generated'\n",
    "max_len = 200 # TODO: Dostoevsky has much longer sentences\n",
    "\n",
    "transformer = Transformer(len(vocab_src), len(vocab_trg), max_len)\n",
    "discriminator = FFN(512, 3, 1024)\n",
    "\n",
    "init_emb_matrix(transformer.encoder.src_word_emb.weight.data, embeddings_src, vocab_src.token2id)\n",
    "init_emb_matrix(transformer.decoder.tgt_word_emb.weight.data, embeddings_trg, vocab_trg.token2id)\n",
    "\n",
    "#####################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, RMSprop\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.umt_batcher import UMTBatcher\n",
    "import src.transformer.constants as constants\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def reconstruction_criterion(vocab_size):\n",
    "    ''' With PAD token zero weight '''\n",
    "    weight = torch.ones(vocab_size)\n",
    "    weight[constants.PAD] = 0\n",
    "\n",
    "    return nn.CrossEntropyLoss(weight)\n",
    "\n",
    "\n",
    "ae_criterion_src = reconstruction_criterion(len(vocab_src))\n",
    "ae_criterion_trg = reconstruction_criterion(len(vocab_trg))\n",
    "translation_criterion_src_to_trg = reconstruction_criterion(len(vocab_trg))\n",
    "translation_criterion_trg_to_src = reconstruction_criterion(len(vocab_src))\n",
    "adv_criterion = nn.BCELoss()\n",
    "\n",
    "transformer_optimizer = Adam(transformer.get_trainable_parameters(), lr=3e-4, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = RMSprop(discriminator.parameters(), lr=5e-4)\n",
    "\n",
    "training_data = UMTBatcher(train_src_idx, train_trg_idx, vocab_src, vocab_trg,\n",
    "                           batch_size=32, shuffle=True)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for batch in tqdm(training_data, mininterval=2, desc='  - (Training)   ', leave=False):\n",
    "    src_noised, trg_noised, src, trg = batch\n",
    "    \n",
    "    # Resetting gradients\n",
    "    transformer_optimizer.zero_grad()\n",
    "    discriminator_optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    ### Training autoencoder ###\n",
    "    transformer.train()\n",
    "    # Computing translation for ~src->src and ~trg->trg autoencoding tasks\n",
    "    print('Training discriminator')\n",
    "    print('Computing predictions')\n",
    "    preds_src, encodings_src = transformer(src_noised, src, return_encodings=True, use_src_embs_in_decoder=True)\n",
    "    preds_trg, encodings_trg = transformer(trg_noised, trg, return_encodings=True, use_trg_embs_in_encoder=True)\n",
    "\n",
    "    print('Computing losses')\n",
    "    ae_loss_src = ae_criterion_src(preds_src, src[:, 1:].contiguous().view(-1))\n",
    "    ae_loss_trg = ae_criterion_trg(preds_trg, trg[:, 1:].contiguous().view(-1))\n",
    "    \n",
    "    print('Computing gradients')\n",
    "    ae_loss_src.backward(retain_graph=True)\n",
    "    ae_loss_trg.backward(retain_graph=True)\n",
    "    \n",
    "    ### Training translator ###\n",
    "    print('Training translator')\n",
    "    transformer.eval()\n",
    "    # Get translations for backtranslation\n",
    "    print('Computing back-translations')\n",
    "    bt_trg, *_ = transformer.translate_batch(src, beam_size=2, max_len=10)\n",
    "    bt_src, *_ = transformer.translate_batch(trg, use_trg_embs_in_encoder=True, use_src_embs_in_decoder=True, beam_size=2, max_len=10)\n",
    "    \n",
    "    bt_trg = Variable(torch.LongTensor(bt_trg))\n",
    "    bt_src = Variable(torch.LongTensor(bt_src))\n",
    "\n",
    "    # We are given n-best translations. Let's pick the best one\n",
    "    bt_trg = bt_trg[:,0,:]\n",
    "    bt_src = bt_src[:,0,:]\n",
    "\n",
    "    if use_cuda:\n",
    "        bt_trg = bt_trg.cuda()\n",
    "        bt_src = bt_src.cuda()\n",
    "    \n",
    "    # Computing predictions for back-translated sentences\n",
    "    transformer.train()\n",
    "    print('Computing predictions (translations of back-translations)')\n",
    "    bt_src_preds = transformer(bt_trg, src, use_trg_embs_in_encoder=True, use_src_embs_in_decoder=True)\n",
    "    bt_trg_preds = transformer(bt_src, trg)\n",
    "    \n",
    "    print('Computing losses')\n",
    "    loss_bt_src = translation_criterion_trg_to_src(bt_src_preds, src[:, 1:].contiguous().view(-1))\n",
    "    loss_bt_trg = translation_criterion_src_to_trg(bt_trg_preds, trg[:, 1:].contiguous().view(-1))\n",
    "    \n",
    "    print('Computing gradients')\n",
    "    loss_bt_src.backward(retain_graph=True)\n",
    "    loss_bt_trg.backward(retain_graph=True)\n",
    "    \n",
    "    print('Updating weights')\n",
    "    transformer_optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Resetting gradients before adversarial update\n",
    "    transformer_optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    ### Training discriminator ###\n",
    "    print('Training discriminator')\n",
    "    print('Computing predictions')\n",
    "    domains_preds_src = discriminator(encodings_src.view(-1, 512))\n",
    "    domains_preds_trg = discriminator(encodings_trg.view(-1, 512))\n",
    "    \n",
    "    # Generating targets for discriminator and generator\n",
    "    true_domains_src = Variable(torch.Tensor([0] * len(domains_preds_src)))\n",
    "    true_domains_trg = Variable(torch.Tensor([1] * len(domains_preds_trg)))\n",
    "    fake_domains_src = Variable(torch.Tensor([1] * len(domains_preds_src)))\n",
    "    fake_domains_trg = Variable(torch.Tensor([0] * len(domains_preds_trg)))\n",
    "\n",
    "    if use_cuda:\n",
    "        true_domains_src = true_domains_src.cuda()\n",
    "        true_domains_trg = true_domains_trg.cuda()\n",
    "        fake_domains_src = fake_domains_src.cuda()\n",
    "        fake_domains_trg = fake_domains_trg.cuda()\n",
    "\n",
    "    # True domains for discriminator loss\n",
    "    print('Computing losses')\n",
    "    discr_loss_src = adv_criterion(domains_preds_src, true_domains_src)\n",
    "    discr_loss_trg = adv_criterion(domains_preds_trg, true_domains_trg)\n",
    "\n",
    "    print('Computing gradients')\n",
    "    discr_loss_src.backward(retain_graph=True)\n",
    "    discr_loss_trg.backward(retain_graph=True)\n",
    "\n",
    "    print('Updating parameters')\n",
    "    discriminator_optimizer.step()\n",
    "\n",
    "    transformer_optimizer.zero_grad()\n",
    "    discriminator_optimizer.zero_grad()\n",
    "\n",
    "    ### Training generator ###\n",
    "    print('Training generator')\n",
    "    print('Computing losses')\n",
    "    # Faking domains for generator loss\n",
    "    gen_loss_src = adv_criterion(domains_preds_src, fake_domains_src)\n",
    "    gen_loss_trg = adv_criterion(domains_preds_trg, fake_domains_trg)\n",
    "\n",
    "    print('Computing gradients')\n",
    "    gen_loss_src.backward(retain_graph=True)\n",
    "    gen_loss_trg.backward(retain_graph=True)\n",
    "\n",
    "    print('Updating parameters')\n",
    "    transformer_optimizer.step()\n",
    "\n",
    "\n",
    "    ### Now, let's compute some statistics and vizualize our staff\n",
    "    losses.append({\n",
    "        'ae_loss_src': ae_loss_src.data[0],\n",
    "        'ae_loss_trg': ae_loss_trg.data[0],\n",
    "        'loss_bt_src': loss_bt_src.data[0],\n",
    "        'loss_bt_trg': loss_bt_trg.data[0],\n",
    "        'discr_loss_src': discr_loss_src.data[0],\n",
    "        'discr_loss_trg': discr_loss_trg.data[0],\n",
    "        'gen_loss_src': gen_loss_src.data[0],\n",
    "        'gen_loss_trg': gen_loss_trg.data[0]  \n",
    "    })\n",
    "\n",
    "    print('Losses:', losses[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
