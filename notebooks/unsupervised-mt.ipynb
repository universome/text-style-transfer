{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should initialize our transformer with learnt embeddings, initialize discriminator and add adversarial loss.\n",
    "When we are done with that — we are only left with training the thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from src.vocab import Vocab\n",
    "from src.transformer.models import Transformer\n",
    "from src.models import FFN\n",
    "from src.utils.data_utils import load_embeddings, init_emb_matrix\n",
    "\n",
    "DATA_PATH = '../data/generated'\n",
    "max_len = 200 # TODO: Dostoevsky has much longer sentences\n",
    "\n",
    "vocab_src = Vocab.from_file(os.path.join(DATA_PATH, 'vocab.en'))\n",
    "vocab_trg = Vocab.from_file(os.path.join(DATA_PATH, 'vocab.de'))\n",
    "\n",
    "transformer = Transformer(len(vocab_src), len(vocab_trg), max_len)\n",
    "discriminator = FFN(512, 3, 1024)\n",
    "\n",
    "# Initializing transformer encoder and decoder with embeddings\n",
    "embeddings_src = load_embeddings('../trained_models/wmt17.en.tok.bpe_cbow.vec')\n",
    "embeddings_trg = load_embeddings('../trained_models/wmt17.de.tok.bpe_cbow.vec')\n",
    "\n",
    "init_emb_matrix(transformer.encoder.src_word_emb.weight.data, embeddings_src, vocab_src.token2id)\n",
    "init_emb_matrix(transformer.decoder.tgt_word_emb.weight.data, embeddings_trg, vocab_trg.token2id)\n",
    "\n",
    "train_src_path = os.path.join(DATA_PATH, 'train.en.tok.bpe')\n",
    "train_trg_path = os.path.join(DATA_PATH, 'train.de.tok.bpe')\n",
    "val_src_path = os.path.join(DATA_PATH, 'val.en.tok.bpe')\n",
    "val_trg_path = os.path.join(DATA_PATH, 'val.de.tok.bpe')\n",
    "\n",
    "train_src = open(train_src_path, 'r', encoding='utf-8').read().splitlines()\n",
    "train_trg = open(train_trg_path, 'r', encoding='utf-8').read().splitlines()\n",
    "val_src = open(val_src_path, 'r', encoding='utf-8').read().splitlines()\n",
    "val_trg = open(val_trg_path, 'r', encoding='utf-8').read().splitlines()\n",
    "\n",
    "train_src = [s.split() for s in train_src]\n",
    "train_trg = [s.split() for s in train_trg]\n",
    "val_src = [s.split() for s in val_src]\n",
    "val_trg = [s.split() for s in val_trg]\n",
    "\n",
    "train_src_idx = [[vocab_src.token2id.get(t, vocab_src.unk) for t in s] for s in train_src]\n",
    "train_trg_idx = [[vocab_trg.token2id.get(t, vocab_trg.unk) for t in s] for s in train_trg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we should write a training procedure, including backtranslation and noising.\n",
    "That's not so easy, as it may seem.\n",
    "Also we should write loss functions and add training visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping  __BOS__\n",
      "Skipping  __EOS__\n",
      "Skipping  __UNK__\n",
      "Skipping  __PAD__\n",
      "Skipping  __BOS__\n",
      "Skipping  __EOS__\n",
      "Skipping  __UNK__\n",
      "Skipping  __PAD__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Training)   :   0%|          | 0/907 [00:00<?, ?it/s]/Users/universome/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py:325: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training discriminator\n",
      "Computing predictions\n",
      "Computing losses\n",
      "Computing gradients\n",
      "Training translator\n",
      "Computing back-translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A../src/transformer/models.py:236: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.44it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:07,  1.08it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.06s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:04<00:06,  1.07s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.11s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:06<00:04,  1.11s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:07<00:03,  1.14s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.17s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.18s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:04,  1.87it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:05,  1.38it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:02<00:05,  1.30it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:03<00:04,  1.24it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:04<00:04,  1.15it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:03,  1.02it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:07<00:03,  1.08s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.26s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:12<00:01,  1.36s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.44s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predictions (translations of back-translations)\n",
      "Computing losses\n",
      "Computing gradients\n",
      "Updating weights\n",
      "Training discriminator\n",
      "Computing predictions\n",
      "Computing losses\n",
      "Computing gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/universome/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([608])) that is different to the input size (torch.Size([608, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/Users/universome/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([544])) that is different to the input size (torch.Size([544, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating parameters\n",
      "Training generator\n",
      "Computing losses\n",
      "Computing gradients\n",
      "Updating parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  - (Training)   :   0%|          | 1/907 [01:06<16:41:19, 66.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ae_loss_src': 8.957502365112305, 'ae_loss_trg': 9.309548377990723, 'loss_bt_src': 8.96019172668457, 'loss_bt_trg': 9.308553695678711, 'discr_loss_src': 0.6752368807792664, 'discr_loss_trg': 0.7115498781204224, 'gen_loss_src': 0.7113943099975586, 'gen_loss_trg': 0.6750862002372742}\n",
      "Training discriminator\n",
      "Computing predictions\n",
      "Computing losses\n",
      "Computing gradients\n",
      "Training translator\n",
      "Computing back-translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:12,  1.44s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:16,  2.06s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:06<00:14,  2.08s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:07<00:11,  1.92s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:09<00:09,  1.86s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:10<00:06,  1.75s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:12<00:05,  1.72s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:14<00:03,  1.80s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:16<00:01,  1.89s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.58it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:06,  1.23it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:02<00:06,  1.16it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:04<00:04,  1.00it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:06<00:04,  1.11s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.27s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:11<00:02,  1.42s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:13<00:01,  1.48s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predictions (translations of back-translations)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-df7b25dfbedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Computing predictions (translations of back-translations)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mbt_src_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt_trg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_trg_embs_in_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_src_embs_in_decoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mbt_trg_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/neuro_dostoevsky/src/transformer/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, use_trg_embs_in_encoder, use_src_embs_in_decoder, return_encodings)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_src_embs_in_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_word_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mseq_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_word_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/neuro_dostoevsky/src/transformer/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt_seq, src_seq, enc_output, embs, return_attns)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mslf_attn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_slf_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 dec_enc_attn_mask=dec_enc_attn_pad_mask)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_attns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/neuro_dostoevsky/src/transformer/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dec_input, enc_output, slf_attn_mask, dec_enc_attn_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m             dec_input, dec_input, dec_input, attn_mask=slf_attn_mask)\n\u001b[1;32m     34\u001b[0m         dec_output, dec_enc_attn = self.enc_attn(\n\u001b[0;32m---> 35\u001b[0;31m             dec_output, enc_output, enc_output, attn_mask=dec_enc_attn_mask)\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyvenvs/zoo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/neuro_dostoevsky/src/transformer/sub_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, attn_mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# treat the result as a (n_head * mb_size) size batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mq_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_qs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n_head*mb_size) x len_q x d_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mk_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_ks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n_head*mb_size) x len_k x d_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mv_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_vs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n_head*mb_size) x len_v x d_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, RMSprop\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.umt_batcher import UMTBatcher\n",
    "import src.transformer.constants as constants\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def reconstruction_criterion(vocab_size):\n",
    "    ''' With PAD token zero weight '''\n",
    "    weight = torch.ones(vocab_size)\n",
    "    weight[constants.PAD] = 0\n",
    "\n",
    "    return nn.CrossEntropyLoss(weight)\n",
    "\n",
    "\n",
    "ae_criterion_src = reconstruction_criterion(len(vocab_src))\n",
    "ae_criterion_trg = reconstruction_criterion(len(vocab_trg))\n",
    "translation_criterion_src_to_trg = reconstruction_criterion(len(vocab_trg))\n",
    "translation_criterion_trg_to_src = reconstruction_criterion(len(vocab_src))\n",
    "adv_criterion = nn.BCELoss()\n",
    "\n",
    "transformer_optimizer = Adam(transformer.get_trainable_parameters(), lr=3e-4, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = RMSprop(discriminator.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def visualize_losses(losses_history, figsize=(15,15)):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=[15,15])\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.title(\"batch loss\")\n",
    "    plt.plot(loss_history)\n",
    "    plt.plot(ewma(np.array(loss_history),span=50))\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title(\"disc loss\")\n",
    "    plt.plot(disc_loss_history)\n",
    "    plt.plot(pd.DataFrame(np.array(disc_loss_history), span=50))\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = UMTBatcher(train_src_idx, train_trg_idx, vocab_src, vocab_trg,\n",
    "                           batch_size=32, shuffle=True)\n",
    "max_num_epochs = 100\n",
    "start_bt_from_epoch = 1\n",
    "num_iters_done = 0\n",
    "losses_history = []\n",
    "    \n",
    "for epoch in range(max_num_epochs):\n",
    "    for batch in tqdm(training_data, leave=False):\n",
    "        try:\n",
    "            should_backtranslate = epoch > start_bt_from_epoch\n",
    "            losses = training_step(batch, should_backtranslate=should_backtranslate)\n",
    "            losses_history.append(losses)\n",
    "            num_iters_done += 1\n",
    "            \n",
    "            # Let's visualize some things\n",
    "            visualize_losses(losses_history)\n",
    "        except KeyboardInterrupt:\n",
    "            should_continue = False\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
