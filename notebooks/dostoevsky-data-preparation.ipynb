{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should preprocess Dostoevsky and News corpora:\n",
    "* split each one into individual sentences;\n",
    "* tokenize each one;\n",
    "* calculate, how much dictionaries diverge (I really hope that they are almost the same)\n",
    "* learn joint BPEs\n",
    "* apply BPE\n",
    "* learn strong embeddings for these BPEs on a joint shuffled corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's joint all Dostoevsky books into single corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "books = []\n",
    "\n",
    "for bookname in os.listdir('../data/dostoevsky'):\n",
    "    book = open(path.join('../data/dostoevsky', bookname), 'r', encoding='utf-8').read().splitlines()\n",
    "    book = [line for line in book if len(line) > 0]\n",
    "    books.append(book)\n",
    "    \n",
    "data = [s for book in books for s in book]\n",
    "\n",
    "with open('../data/generated/dostoevsky_joined.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in data:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split sentences in Dostoevsky (there can be very long sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence Splitter v3\n",
      "Language: ru\n",
      "bash: line 10: !wc: command not found\n",
      "bash: line 11: !wc: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dostoevsky_joined=\"../data/generated/dostoevsky_joined.txt\"\n",
    "dostoevsky_sent_split=\"../data/generated/dostoevsky_sent_split.txt\"\n",
    "mosesdecoder=\"../ext-libs/mosesdecoder\"\n",
    "\n",
    "$mosesdecoder/scripts/ems/support/split-sentences.perl -l ru \\\n",
    "    < $dostoevsky_joined > $dostoevsky_sent_split\n",
    "\n",
    "# Let's compute, how much more sentences we got\n",
    "wc -l ../data/generated/dostoevsky_joined.txt\n",
    "wc -l ../data/generated/dostoevsky_sent_split.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer Version 1.1\n",
      "Language: ru\n",
      "Number of threads: 6\n",
      "Tokenizer Version 1.1\n",
      "Language: ru\n",
      "Number of threads: 6\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mosesdecoder=\"../ext-libs/mosesdecoder\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "\n",
    "threads=6\n",
    "\n",
    "cat \"$generated_data_dir/dostoevsky_sent_split.txt\" | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads -l ru > \\\n",
    "    $generated_data_dir/dostoevsky.tok\n",
    "    \n",
    "cat \"$data_dir/news.2016.ru.shuffled\" | \\\n",
    "    $mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l ru | \\\n",
    "    $mosesdecoder/scripts/tokenizer/tokenizer.perl -threads $threads -l ru > \\\n",
    "    $generated_data_dir/news.ru.tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see, how much tokens we share in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of news vocabulary 1326362\n",
      "Size of Dostoevsky vocabulary 128977\n",
      "How much tokens intersect? 94632\n"
     ]
    }
   ],
   "source": [
    "from src.vocab import Vocab\n",
    "\n",
    "news = open('../data/generated/news.ru.tok', encoding='utf-8').read().splitlines()\n",
    "dostoevsky = open('../data/generated/dostoevsky.tok', encoding='utf-8').read().splitlines()\n",
    "\n",
    "vocab_news = Vocab.from_sequences(news)\n",
    "vocab_dostoevsky = Vocab.from_sequences(dostoevsky)\n",
    "\n",
    "vocab_news = set(vocab_news.token2id.keys())\n",
    "vocab_dostoevsky = set(vocab_dostoevsky.token2id.keys())\n",
    "\n",
    "print('Size of news vocabulary', len(vocab_news))\n",
    "print('Size of Dostoevsky vocabulary', len(vocab_dostoevsky))\n",
    "print('How much tokens intersect?', len(vocab_news.intersection(vocab_dostoevsky)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we learn joint BPE, we should first join two corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "joint = news + dostoevsky\n",
    "random.shuffle(joint)\n",
    "\n",
    "with open('../data/generated/dostoevsky-news.tok', 'w', encoding='utf-8') as out_f:\n",
    "    for line in joint:\n",
    "        out_f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argh, now we can learn and apply BPEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "subword_nmt=\"../ext-libs/subword-nmt\"\n",
    "data_dir=\"../data\"\n",
    "generated_data_dir=\"$data_dir/generated\"\n",
    "dataset=\"$generated_data_dir/dostoevsky-news.tok\"\n",
    "num_bpes=10000\n",
    "\n",
    "# bpes=\"../data/generated/bpes\"\n",
    "bpes=\"$generated_data_dir/dostoevsky-news.bpes\"\n",
    "vocab=\"$generated_data_dir/dostoevsky-news.vocab\"\n",
    "python \"$subword_nmt/learn_bpe.py\" -s $num_bpes < $dataset > $bpes\n",
    "\n",
    "# Let's apply bpe here for our tokenized files\n",
    "for file in \"news.ru.tok\" \"dostoevsky.tok\" \"dostoevsky-news.tok\"\n",
    "do\n",
    "    python \"$subword_nmt/apply_bpe.py\" -c $bpes \\\n",
    "        < $generated_data_dir/$file > $generated_data_dir/$file.bpe\n",
    "done\n",
    "\n",
    "# And finally, we should generate vocab\n",
    "python \"$subword_nmt/get_vocab.py\" < $bpes > $vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, actually it's not very correct to learn embeddings this way, because we have 7m (1.4G) lines of News corpora and only 100k (20mb) lines of Dostoevsky. But, let's try to do it this way anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "num_threads = 6\n",
    "\n",
    "model_trg = fasttext.skipgram('../data/generated/dostoevsky-news.tok.bpe',\n",
    "                              '../trained_models/dostoevsky-news.tok.bpe.skipgram',\n",
    "                              dim=512, min_count=1, silent=0, thread=num_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
