{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from firelab import BaseTrainer\n",
    "from firelab.utils import cudable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.data import Field, Dataset, Example\n",
    "\n",
    "batch_size = 16 #self.config.get('batch_size', 8)\n",
    "project_path = '/home/skorokhodov/neuro_dostoevsky'\n",
    "\n",
    "get_data_path = lambda x: os.path.join(project_path, 'data/shakespeare/%s.split.tok' % x)\n",
    "modern_data_path = get_data_path('modern')\n",
    "original_data_path = get_data_path('original')\n",
    "\n",
    "with open(modern_data_path) as f: modern = f.read().splitlines()\n",
    "with open(original_data_path) as f: original = f.read().splitlines()\n",
    "    \n",
    "text = Field(init_token='<bos>', eos_token='<eos>', batch_first=True)\n",
    "fields = [('modern', text), ('original', text)]\n",
    "examples = [Example.fromlist([m,o], fields) for m,o in zip(modern, original)]\n",
    "\n",
    "dataset = Dataset(examples, fields)\n",
    "text.build_vocab(dataset)\n",
    "data_iter = data.BucketIterator(dataset, batch_size, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, emb_size, hid_size, vocab_size):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "\n",
    "        self.hid_size = hid_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.gru = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "        \n",
    "        self.style_nn = nn.Sequential(\n",
    "            nn.Linear(hid_size//2, hid_size//2),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.content_nn = nn.Sequential(\n",
    "            nn.Linear(hid_size//2, hid_size//2),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        _, last_hidden_state = self.gru(embeds)\n",
    "        state = last_hidden_state.squeeze(0)\n",
    "        \n",
    "        style = self.style_nn(state[:, :self.hid_size//2])\n",
    "        content = self.content_nn(state[:, self.hid_size//2:])\n",
    "\n",
    "        return style, content\n",
    "\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, emb_size, hid_size, vocab_size):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "\n",
    "        self.hid_size = hid_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.gru = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "        self.embs_to_logits = nn.Linear(hid_size, vocab_size)\n",
    "        # self.embs_to_logits.weight = self.embeddings.weight # Sharing weights\n",
    "\n",
    "    def forward(self, z, sentences):\n",
    "        embs = self.embeddings(sentences)\n",
    "        hid_states, _ = self.gru(embs, z.unsqueeze(0))\n",
    "        logits = self.embs_to_logits(hid_states)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(size, size),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 512\n",
    "hid_size = 512\n",
    "voc_size = len(text.vocab)\n",
    "\n",
    "encoder = cudable(RNNEncoder(emb_size, hid_size, voc_size))\n",
    "decoder = cudable(RNNDecoder(emb_size, hid_size, voc_size))\n",
    "critic = cudable(MLP(hid_size // 2))\n",
    "motivator = cudable(MLP(hid_size // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define losses and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Reconstruction loss\n",
    "weights = cudable(torch.ones(voc_size))\n",
    "weights[text.vocab.stoi['<pad>']] = 0\n",
    "rec_criterion = nn.CrossEntropyLoss(weights)\n",
    "\n",
    "# Critic loss. Is similar to WGAN (but without lipschitz constraints)\n",
    "class CriticLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, real, fake):\n",
    "        return real.mean() - fake.mean()\n",
    "\n",
    "critic_criterion = CriticLoss()\n",
    "\n",
    "# Motivator loss\n",
    "motivator_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizers\n",
    "critic_optim = Adam(critic.parameters(), lr=1e-4)\n",
    "motivator_optim = Adam(motivator.parameters(), lr=1e-4)\n",
    "ae_optim = Adam(chain(encoder.parameters(), decoder.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.47\n"
     ]
    }
   ],
   "source": [
    "for batch in data_iter:\n",
    "    # Computing codes we need\n",
    "    style_modern, content_modern = encoder(batch.modern)\n",
    "    style_original, content_original = encoder(batch.original)\n",
    "    \n",
    "    # Now we should merge back style and content for decoder\n",
    "    hid_modern = torch.cat([style_modern, content_modern], dim=1)\n",
    "    hid_original = torch.cat([style_original, content_original], dim=1)\n",
    "    \n",
    "    # Ok, we now have all codes that we want\n",
    "    # First, let's decode and compute reconstruction loss\n",
    "    recs_modern = decoder(hid_modern, batch.modern[:, :-1])\n",
    "    recs_original = decoder(hid_original, batch.original[:, :-1])\n",
    "    \n",
    "    rec_loss_modern = rec_criterion(recs_modern.view(-1, voc_size), batch.modern[:, 1:].contiguous().view(-1))\n",
    "    rec_loss_original = rec_criterion(recs_original.view(-1, voc_size), batch.original[:, 1:].contiguous().view(-1))\n",
    "    rec_loss = rec_loss_modern + rec_loss_original\n",
    "    \n",
    "    # Computing critic loss\n",
    "    critic_loss = critic_criterion(content_modern, content_original)\n",
    "    \n",
    "    # Computing motivator loss\n",
    "    motivator_logits_modern = motivator(style_modern)\n",
    "    motivator_logits_original = motivator(style_original)\n",
    "    motivator_loss_modern = motivator_criterion(motivator_logits_modern, torch.ones_like(motivator_logits_modern))\n",
    "    motivator_loss_original = motivator_criterion(motivator_logits_original, torch.zeros_like(motivator_logits_original))\n",
    "    motivator_loss = motivator_loss_modern + motivator_loss_original\n",
    "    \n",
    "    # Now we can make backward passes\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    critic_optim.step()\n",
    "\n",
    "    ae_optim.zero_grad()\n",
    "    motivator_optim.zero_grad()\n",
    "    motivator_loss.backward(retain_graph=True)\n",
    "    motivator_optim.step()\n",
    "    ae_optim.step()\n",
    "    \n",
    "    ae_optim.zero_grad()\n",
    "    rec_loss.backward(retain_graph=True)\n",
    "    ae_optim.step()\n",
    "    \n",
    "    total_loss = rec_loss + critic_loss + motivator_loss\n",
    "    print('Loss: {:.02f}'.format(total_loss.item()))\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
